\section{Hierarchically Fused Fully Convolutional Network}
\label{Sec:HF-FCN}
In this paper, we propose a general operation for feature fusion, named hierarchical fusion operation and apply it to the common networks, VGG16 Net and ResNet. The experiments show that the fusion operation improve the segmentation results. The overview diagram in Fig.~\ref{fig:Fusion-Operation} shows where the fusion operations take effect and how they work. Our network consists of three parts. The first part is a backbone network whose role is to extract the features at different levels. In theory, arbitrary feature extraction network is applicable to the first part. The second part is feature fusion of first stage, which fuses the feature maps generated from each convolutional(conv) layer. Besides, the last part is feature fusion of second stage. In the second stage of the fusion process, we take full advantage of the information extracted from the second part by learning the connection weights between layers.

\begin{figure}
\centering
\includegraphics[width=8.5cm]{Figures/Fusion-Operation.eps}
\caption{First row shows how the fusion operation used in a general network. Second row shows the details of fusion operation.}
\label{fig:Fusion-Operation}
\end{figure}
\cxj{Put overview here. Explain the main components of our methods.}
\subsection{Network Architecture}
 Here, we illustrate our main idea using the VGG16 network as our backbone network, which have been proven to have better performance in experiments.
 Some modifications are made to apply to our building extraction task including removing its fc layers and last pooling layer. The reasons of these changes are 
 1) The fc layer generates a fair number of parameters and takes up too much memory. 
 2) The existence of fc layer limits the size of input image. 
 3) After the last pooling layer, the resolution of the feature map is reduced to 1/32 of the input, which is too small to building extraction task.
 The details of our network are shown in Fig.~\ref{fig:network_architecture}. The Level 1 in Fig.~\ref{fig:network_architecture} is a trimmed VGG16 Net which regards as our first part: backbone network.
 \cxj{Where do you define F1\_1? Is it the layer of VGG16 or HF-FCN?}
 %

In order to leverage the information extracted from different layers, we add the fusion branches on the backbone network which fuses the prediction results obtained by different feature maps.
The idea is similar to getting the response of scale functions of images when looking for the SIFT feature points. 
After getting the responses of different scale functions, the biggest response is selected between adjacent scales of each feature point. 
Choosing the most suitable scale is determined by weights learned from fusion operations in our network. 
In our view, the fusion operations in first stage play a role of both feature compression and semantic information extraction which extracts the information from different scales of receptive field as well as diverse levels of semantics.
In addition, the whole weights are learned from the network automatically indicating that network studies the connection relationship among feature maps.
The fusion operations between Level 1 and Level 2 in Fig.~\ref{fig:network_architecture} form the second part of our network.\cxj{Fig 1 is the introduction figure.}
With the growing of the receptive field, the detailed information is captured by each conv layer from fine-grained to coarser while the semantic information captured from low level to high level.
For the task of rooftop extraction, not only the details of the appearance of the buildings captured by shallow layers is needed, but also the line and corner extracted by middle layers and the high-level semantics which mainly come from deep layers are needed.
Therefore, we extract various kinds of information by applying the fusion operations to the whole conv layers. 
The upsampled feature maps from different conv layers are shown in Fig.~\ref{fig:feature_maps}. 
The U1\_1 in Fig.~\ref{fig:feature_maps}(b) means the upsampled feature maps from F1\_1 which are feature maps generated from conv1\_1 with small receptive field extracts low-level features like edges.
In Fig.~\ref{fig:feature_maps}(c), the U1\_2 looks like an over-segmentation which groups pixels with similar color or texture into a subregion.
In the U2\_1, as Fig.~\ref{fig:feature_maps}(d) shows, shape information is augmented.
From the U3\_3, we can see that regions with significantly varying appearance are merged into an integrated building by considering high-level features.
In U4\_3 and U5\_3, more semantic information of rooftop is got, which can distinguish the rooftop and the roads with similar color and deal with the problem caused by shadow. 
%

\begin{figure*}
\centering
\includegraphics[width=18cm,height=9.5cm]{Figures/network_architecture.eps}
\centering
\caption{Overall architecture of HF-FCN. The action of VGG16 network is extracting features from various layers. All feature maps from various layers merge into a feature map which eliminating redundancy in the same semantics and resolution. In Level 2, 13 feature maps are up-sampled and cropped to the same size of input. Finally the cropped feature maps fuse into a prediction result. All fusion operations used in HF-FCN are 1$\times$1 convolution. The input channel could be 3, 4 or 5 for RGB, DSM, nDSM. $\times$2 means 2 times of upper sampling. U1\_1 means the upsampling of F1\_1, and so forth. \cxj{What is the different between our network with U-Net or other FCN networks?} }
\label{fig:network_architecture}
\end{figure*}

After getting the upsampled feature maps, we 
Since all the upsampled feature maps are fused, it is expected to achieve a boost in rooftop segmentation. The final prediction result is shown in Fig.~\ref{fig:feature_maps}(h).
In Level 1, 
Hence, in Level 2, we combine hierarchical features from whole up-sampling layers into a final prediction. Fig.~\ref{fig:feature_maps} shows the upsampled feature maps from different convolutional layers.

\begin{figure}
\centering
\includegraphics[width=8.7cm]{Figures/feature_maps.eps}
\caption{(a) Input aerial image. (b-g) Feature maps generated from U1\_1, U1\_2, U2\_2, U3\_3, U4\_3, U5\_3, respectively. (h) Predicted label map.}
\label{fig:feature_maps}
\end{figure}


\subsection{Network Training}

The ground truth $M$ in our dataset is labeled by 0 or 1 to indicate whether a pixel belongs to a roof or not. \cxj{only roof? or part of the building including facades?}
When a remote sensing image ${X}$ is inputted into the network, the output is a prediction probability map $P(X;W)$ of roof, where $W$ denotes all the parameters that learned by HF-FCN. Each pixel value in $P(X_{i};W)$ means the probability of this pixel belongs to rooftop.
We use the sigmoid cross-entropy loss function formulated as
\begin{small}
\begin{equation}
     \label{loss}
     \ L(W)\! =\! -\frac{1}{\vert I\vert}\sum_{i=1}^{\vert I \vert}\lbrack{\tilde{m}_i \log{P(X_{i};W)}\!+\!(1\!-\!\tilde{m}_i)\log(1\!-\!P(X_{i};W)}\rbrack,
\end{equation}
\end{small}
where $\tilde{m}_i$ is label of $X_{i}$, ${\vert I\vert}$ is the number of pixels in the input image ${X}$.
